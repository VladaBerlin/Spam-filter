import string
import math
import nltk
import pymorphy2
import os
from nltk.tokenize import wordpunct_tokenize
from pymorphy2 import MorphAnalyzer
import numpy as np
from nltk.corpus import stopwords
from sklearn.feature_extraction.text import TfidfVectorizer
import mailbox, re
from bs4 import BeautifulSoup


def get_top_tf_idf_words(tfidf_vector, feature_names, top_n):
    sorted_nzs = np.argsort(tfidf_vector.data)[:-(top_n+1):-1]
    keys = feature_names[tfidf_vector.indices[sorted_nzs]]
    values = tfidf_vector.indices[sorted_nzs]
    dct = {}
    for k in range(len(keys)):
        dct[keys[k]] = values[k]
    return dct


def punct_triggers(message):
    exclamation_marks = 0
    punctuation_marks = 0
    for symbol in message:
        if symbol == '!':
            exclamation_marks += 1
        if symbol in string.punctuation:
            punctuation_marks += 1
    if punctuation_marks == 0:
        return 0
    elif exclamation_marks / punctuation_marks >= 0.5:
        return 1 #надо уточнить вес
    else:
        return 0


def bayes_spam(the_list): # собственно наивный байесовский классификатор
    result = 0
    for word in the_list:
        nik = 0
        if word in spam_triggers.keys():
            nik = spam_triggers[word]
        spam_probability = (a + nik) / (a * spam_m + spam_nk)
        result += math.log10(spam_probability)
    return result


def bayes_notspam(the_list):
    result = 0
    for word in the_list:
        nik = 0
        if word in notspam_triggers.keys():
            nik = notspam_triggers[word]
        notspam_probability = (a + nik) / (a * notspam_m + notspam_nk)
        result += math.log10(notspam_probability)
    return result


def addressing(text): #наличие обращения по имени в начале
    prob_thresh = 0.4
    address = False
    morph = pymorphy2.MorphAnalyzer()
    i = 0
    while i <= 7 and i <= len(nltk.word_tokenize(text)) - 1:
        word = nltk.word_tokenize(text)[i]
        for p in morph.parse(word):
            if 'Name' in p.tag and p.score >= prob_thresh:
                address = True
        i += 1
    if not address:
        return 0.5 #надо уточнить вес
    else:
        return 0


def greeting(message): #наличие приветствия в начале
    fg = False
    greetings = ['привет', 'здравствуйте', 'здравствуй', 'доброе утро', 'добрый день', 'добрый вечер',
                 'доброго времени суток', 'уважаемый', 'уважаемая', 'дорогой', 'дорогая', 'дорогие', 'уважаемые']
    for elem in greetings:
        if elem in message[:8]:
            fg = True
            continue
    if not fg:
        return 0.3 #надо уточнить вес
    else:
        return 0


mbox = mailbox.mbox(r"Спам.mbox")

for i, message in enumerate(mbox):
    content = message.get_payload(decode=True)
    if content:
        text = content.decode("utf-8", "replace")
        soup = BeautifulSoup(text, 'html.parser')
        item = soup.span
        if item:
            item = item.contents
            for elem in item:
                elem = str(elem).strip()
                elem_clear = re.sub(r'<[^<]+>', '', elem)
                with open('spam_file.txt', 'a', encoding='utf-8') as new_file:
                    new_file.write(elem_clear)

path_to_articles = 'C:\Lessons\проект'
articles = [os.path.join(path_to_articles, item)
    for item in os.listdir(path_to_articles)
    if item.endswith('.txt') and not item != 'text.txt'] #text.txt название входного файла

articles_texts = []

for article_path in articles:
    with open(article_path, encoding='utf8') as a_src:
        articles_texts.append("\n".join(a_src.readlines()))

morph = MorphAnalyzer()

spam_m = 0 #количество слов в обучающей выборке
spam_nk = 0 #количество слов в обучающей выборке без стоп-слов
stops = stopwords.words("russian")
articles_preprocessed = []
for a_text in articles_texts:
    a_tokens = wordpunct_tokenize(a_text)
    a_lemmatized = ' '.join([morph.parse(item)[0].normal_form for item in a_tokens])
    articles_preprocessed.append(a_lemmatized)
    for token in a_tokens:
        p = morph.parse(token)[0]
        if p.tag.POS:
            spam_m += 1
        if p.tag.POS and token not in stops:
            spam_nk += 1

tfidf = TfidfVectorizer(analyzer="word", stop_words=stops)

articles_tfidf = tfidf.fit_transform(articles_preprocessed)

feature_names = np.array(tfidf.get_feature_names())

spam_triggers = {}

for i, article in enumerate(articles_texts):
    article_vector = articles_tfidf[i, :]
    if i == 18: #проблемный кусок - большой файл читается не последним, хотя в папке по алфавиту последний...
        words = get_top_tf_idf_words(article_vector, feature_names, 102)
    else:
        words = get_top_tf_idf_words(article_vector, feature_names, 5)
    if words:
        for key in words:
            t = morph.parse(key)[0]
            if 'Name' in t.tag and t.score >= 0.4:
                continue
            if key not in spam_triggers.keys():
                spam_triggers[key] = words[key]
            else:
                spam_triggers[key] += words[key]

notspam_triggers = {'яблоки':3, 'магазин':3, 'отправляю':6, 'банк': 5}
#пока придуманные циферки для расчетов:
a = 1 #параметр для сглаживания (чтобы не получить вероятность 0)
notspam_m = 45
notspam_nk = 31

message = ''
filename = text.txt
with open (filename, encoding='utf-8') as f:
    for line in f:
        if line == '':
            continue
        else:
            message = message + line.strip() + ' '

punct = punct_triggers(message)
for elem in string.punctuation: #очистить текст письма от пунктуации
    message = message.replace(elem, "")

address = addressing(message)
message = message.lower()
greet = greeting(message)
message = message.split()
p_spam = bayes_spam(message) + punct + address + greet
p_notspam = bayes_notspam(message)
#print(address, greet)
print(p_spam, p_notspam)

if p_spam > p_notspam:
    print('это спам')
else:
    print('это не спам')
